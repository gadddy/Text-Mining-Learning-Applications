{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import contractions\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('unique_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data with new column where review_stars is greater than 3 is positive and less than 3 is negative and equal to 3 is neutral\n",
    "df['sentiment'] = df['review_stars'].apply(lambda x: 'positive' if x > 3 else 'negative' if x < 3 else 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# We use the following list to store the sequence of sentence labels.\n",
    "labels = []\n",
    "\n",
    "# We use the following list to store the sentences, where each sentence itself is a list of words.\n",
    "corpus = []\n",
    "\n",
    "# For every row in the data frame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the label and the text.\n",
    "    label = row['sentiment']\n",
    "    text = row['review_text']\n",
    "    \n",
    "    # Store the label into the list of labels.\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Tokenize the text.\n",
    "    sent = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase conversion\n",
    "    sent = [w.lower() for w in sent]\n",
    "    \n",
    "    # Stop word removal \n",
    "    sent = [w for w in sent if w not in stop_list]\n",
    "\n",
    "    # Remove punctuation\n",
    "    sent = [w for w in sent if w.isalnum()]\n",
    "    \n",
    "    # Lemmatization \n",
    "    sent = [lemmatizer.lemmatize(w) for w in sent]\n",
    "\n",
    "    # Expand contractions\n",
    "    sent = [contractions.fix(w) for w in sent]\n",
    "\n",
    "    # Create bigrams\n",
    "    bigrams = [' '.join(w) for w in list(ngrams(sent, 2))]\n",
    "    sent.extend(bigrams)\n",
    "    \n",
    "    # Store the sentence into the corpus.\n",
    "    corpus.append(sent)\n",
    "\n",
    "\n",
    "print('Finished reading sentences.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing the data.\n",
      "['positive', ['belgian', 'cafe', 'turned', 'someone', 'would', 'barely', 'look', 'mussel', 'someone', 'actually', 'eat', 'occasion', 'get', 'high', 'mark', 'french', 'fry', 'dipping', 'sauce', 'also', 'praised', 'course', 'serve', 'favorite', 'monk', 'sour', 'flemish', 'ale', 'overall', 'good', 'neighborhood', 'spot', 'meet', 'friend', 'drink', 'dinner', 'take', 'good', 'book', 'treat', 'belgian cafe', 'cafe turned', 'turned someone', 'someone would', 'would barely', 'barely look', 'look mussel', 'mussel someone', 'someone actually', 'actually eat', 'eat occasion', 'occasion get', 'get high', 'high mark', 'mark french', 'french fry', 'fry dipping', 'dipping sauce', 'sauce also', 'also praised', 'praised course', 'course serve', 'serve favorite', 'favorite monk', 'monk sour', 'sour flemish', 'flemish ale', 'ale overall', 'overall good', 'good neighborhood', 'neighborhood spot', 'spot meet', 'meet friend', 'friend drink', 'drink dinner', 'dinner take', 'take good', 'good book', 'book treat']]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the corpus.\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "# Store the labeled training data in the following list.\n",
    "labeled_data = []\n",
    "count = 0\n",
    "# Going through the two lists in parallel to create the labeled data set.\n",
    "for (l, s) in zip(labels, corpus):\n",
    "    \n",
    "    # Add the labeled sentence to the labeled data set.\n",
    "    labeled_data.append([l,s])\n",
    "    \n",
    "print('Finished preparing the data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting the data.\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets using 60%, 20%, 20% respectively.\n",
    "train_size = int(len(labeled_data) * 0.6)\n",
    "val_size = int(len(labeled_data) * 0.2)\n",
    "train_data = labeled_data[:train_size]\n",
    "val_data = labeled_data[train_size:train_size+val_size]\n",
    "test_data = labeled_data[train_size+val_size:]\n",
    "\n",
    "print('Finished splitting the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "pos_lexicon = 'positive-words.txt'\n",
    "neg_lexicon = 'negative-words.txt'\n",
    "\n",
    "# Read the positive sentiment lexicon.\n",
    "pos_dict = {}\n",
    "f = open(pos_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    pos_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the negative sentiment lexicon.\n",
    "neg_dict = {}\n",
    "f = open(neg_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    neg_dict[line] = 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following list stores the predicted labels.\n",
    "predicted_labels = []\n",
    "for record in test_data:\n",
    "    score = 0\n",
    "    for w in record[1]:\n",
    "        if w in pos_dict:\n",
    "            score = score + 1\n",
    "        # If the word w is inside the negative lexicon, then decrease the score by 1.\n",
    "        # elif means \"else if\"\n",
    "        elif w in neg_dict:\n",
    "            score = score - 1\n",
    "    if score > 0:\n",
    "        predicted_labels.append('positive')\n",
    "    elif score < 0:\n",
    "        predicted_labels.append('negative')\n",
    "    else:\n",
    "        predicted_labels.append('neutral')   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7746781115879828\n",
      "F1 Score: 0.7273501263941857\n",
      "\n",
      "Lexicon Positive Reviews:  425\n",
      "Lexicon Negative Reviews:  25\n",
      "\n",
      "Actual Positive Reviews:  366\n",
      "Actual Negative Reviews:  29\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the predicted labels.\n",
    "total = len(test_data)\n",
    "correct = 0\n",
    "\n",
    "test_data_labels = []\n",
    "for i in test_data:\n",
    "    test_data_labels.append(i[0])\n",
    "\n",
    "for (tl, pl) in zip(test_data_labels, predicted_labels):\n",
    "    if tl == pl:\n",
    "        correct = correct + 1\n",
    "\n",
    "accu = correct / total\n",
    "print(\"Accuracy: \", accu)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(test_data_labels, predicted_labels, labels=[\"positive\", \"negative\", \"neutral\"], average='weighted')\n",
    "\n",
    "print(\"F1 Score:\", f1)\n",
    "print()\n",
    "print(\"Lexicon Positive Reviews: \", predicted_labels.count('positive'))\n",
    "print(\"Lexicon Negative Reviews: \", predicted_labels.count('negative'))\n",
    "print()\n",
    "print(\"Actual Positive Reviews: \", test_data_labels.count('positive'))\n",
    "print(\"Actual Negative Reviews: \", test_data_labels.count('negative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "pos_lexicon = 'positive-words.txt'\n",
    "neg_lexicon = 'negative-words.txt'\n",
    "\n",
    "# Read the positive sentiment lexicon.\n",
    "pos_dict = {}\n",
    "f = open(pos_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    pos_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the negative sentiment lexicon.\n",
    "neg_dict = {}\n",
    "f = open(neg_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    neg_dict[line] = 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon(test):\n",
    "    \n",
    "    # Preprocessing \n",
    "    sent = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase conversion\n",
    "    sent = [w.lower() for w in sent]\n",
    "    \n",
    "    # Stop word removal \n",
    "    sent = [w for w in sent if w not in stop_list]\n",
    "\n",
    "    # Remove punctuation\n",
    "    sent = [w for w in sent if w.isalnum()]\n",
    "    \n",
    "    # Lemmatization \n",
    "    sent = [lemmatizer.lemmatize(w) for w in sent]\n",
    "\n",
    "    # Expand contractions\n",
    "    sent = [contractions.fix(w) for w in sent]\n",
    "\n",
    "    # Create bigrams\n",
    "    bigrams = [' '.join(w) for w in list(ngrams(sent, 2))]\n",
    "    sent.extend(bigrams)\n",
    "    \n",
    "    words = test.split()\n",
    "    \n",
    "    # Lexicon\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if word in pos_dict:\n",
    "            score += 1\n",
    "        elif word in neg_dict:\n",
    "            score -= 1\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(lexicon(\"BEST experience I have ever had at the KT on 6/14/2015.  John, our server, was amazing.  Really awesome.  The food was awesome as well.  I was sooooo happy we went.  Great job guys!!  Keep up the great work.  BTW the burgers are amazing.  Thanks again!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
